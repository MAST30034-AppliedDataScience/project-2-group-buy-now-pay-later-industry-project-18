{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression to Forecast Merchant Monthly Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/19 14:33:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialise a spark session\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"RF Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Increase driver memory\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # Increase executor memory\n",
    "    .config(\"spark.executor.instances\", \"4\")  # Increase the number of executor instances\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction file\n",
    "transactions = spark.read.parquet('../data/curated/flagged_fraud')\n",
    "transactions = transactions.filter(F.col(\"is_fraud\") != True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- year_week: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- consumer_id: long (nullable = true)\n",
      " |-- fraud_probability_consumer: double (nullable = true)\n",
      " |-- name_consumer: string (nullable = true)\n",
      " |-- address_consumer: string (nullable = true)\n",
      " |-- state_consumer: string (nullable = true)\n",
      " |-- postcode_consumer: integer (nullable = true)\n",
      " |-- gender_consumer: string (nullable = true)\n",
      " |-- name_merchant: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- fraud_probability_merchant: double (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- order_month_year: string (nullable = true)\n",
      " |-- SA4_CODE_2011: string (nullable = true)\n",
      " |-- SA4_NAME_2011: string (nullable = true)\n",
      " |-- unemployment_rate: string (nullable = true)\n",
      " |-- z_score: double (nullable = true)\n",
      " |-- consumer_weekly_transaction: long (nullable = true)\n",
      " |-- merchant_weekly_transaction: long (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating monthly revenue for each merchant\n",
    "monthly_revenue_df = transactions.groupBy('merchant_abn', 'order_month_year', 'name_merchant').agg(\n",
    "    F.sum('dollar_value').alias('monthly_revenue'),\n",
    "    F.count('order_id').alias('transaction_count'),\n",
    "    F.avg('fraud_probability_merchant').alias('avg_fraud_probability_merchant'),\n",
    "    F.first('tags').alias('merchant_tags')  # Assuming tags are constant per merchant\n",
    ")\n",
    "    \n",
    "# Aggregating consumer-level features (most common state and gender for each merchant)\n",
    "\n",
    "# Most common consumer state per merchant\n",
    "consumer_state_mode = transactions.groupBy('merchant_abn', 'state_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'state_consumer')\n",
    "\n",
    "# Most common consumer gender per merchant\n",
    "consumer_gender_mode = transactions.groupBy('merchant_abn', 'gender_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'gender_consumer')\n",
    "\n",
    "# Average Unemployment Rate per Merchant Month-Year\n",
    "transactions = transactions.withColumn(\"unemployment_rate_numeric\", F.col(\"unemployment_rate\").cast(\"float\"))\n",
    "\n",
    "unemployment_agg = transactions.groupBy('merchant_abn', 'order_month_year').agg(\n",
    "    F.avg('unemployment_rate_numeric').alias('avg_unemployment_rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "|merchant_abn|order_month_year|       name_merchant|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|\n",
      "+------------+----------------+--------------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "| 10023283211|          Aug-21|       Felis Limited|15807.479921460477|               86|              56.2429773174978|((furniture, home...|           NSW|           Male|    80.79883797224178|\n",
      "| 10023283211|          Mar-21|       Felis Limited| 9076.307821688919|               40|             56.40749878739966|((furniture, home...|           NSW|           Male|     78.1724992275238|\n",
      "| 10023283211|          May-21|       Felis Limited|11953.898144671877|               47|             55.55771129303114|((furniture, home...|           NSW|           Male|    77.15957534059565|\n",
      "| 10142254217|          Feb-21|Arcu Ac Orci Corp...| 82.56213974011379|                2|             57.58330762068199|([cable, satellit...|           NSW|           Male|    63.44999885559082|\n",
      "| 10142254217|          May-21|Arcu Ac Orci Corp...|2031.1632147275016|               68|             55.92988477860093|([cable, satellit...|           NSW|           Male|    74.11911787706264|\n",
      "+------------+----------------+--------------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining Datasets\n",
    "monthly_revenue_df = monthly_revenue_df.join(consumer_state_mode, on='merchant_abn', how='left') \\\n",
    "                                      .join(consumer_gender_mode, on='merchant_abn', how='left')\n",
    "\n",
    "# Join with unemployment data on both 'merchant_abn' and 'order_month_year'\n",
    "monthly_revenue_df = monthly_revenue_df.join(unemployment_agg, on=['merchant_abn', 'order_month_year'], how='left')\n",
    "\n",
    "# Show the final dataframe\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/19 14:34:06 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------+---------------+-----------------+------------------------------+-------------+--------------+---------------+---------------------+\n",
      "|merchant_abn|order_month_year|name_merchant|monthly_revenue|transaction_count|avg_fraud_probability_merchant|merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|\n",
      "+------------+----------------+-------------+---------------+-----------------+------------------------------+-------------+--------------+---------------+---------------------+\n",
      "|           0|               0|            0|              0|                0|                             0|            0|             0|              0|                    0|\n",
      "+------------+----------------+-------------+---------------+-----------------+------------------------------+-------------+--------------+---------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for Missing Values\n",
    "nulls = monthly_revenue_df.agg(\n",
    "    *(F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in monthly_revenue_df.columns)\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "|merchant_abn|order_month_year|name_merchant|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|previous_month_revenue|      revenue_growth|\n",
      "+------------+----------------+-------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "| 10023283211|          Apr-21|Felis Limited|   9221.4058068711|               47|             56.03849374950703|((furniture, home...|           NSW|           Male|    74.54042625427246|                   0.0|                 0.0|\n",
      "| 10023283211|          Aug-21|Felis Limited|15807.479921460477|               86|              56.2429773174978|((furniture, home...|           NSW|           Male|    80.79883797224178|       9221.4058068711|  0.7142158421964174|\n",
      "| 10023283211|          Dec-21|Felis Limited| 66067.74432316715|              316|             55.74664488810393|((furniture, home...|           NSW|           Male|    76.78892362570461|    15807.479921460477|   3.179524165232218|\n",
      "| 10023283211|          Feb-22|Felis Limited|48572.882608193504|              215|            56.069422789172336|((furniture, home...|           NSW|           Male|    71.28418544059576|     66067.74432316715|-0.26480186200089384|\n",
      "| 10023283211|          Jan-22|Felis Limited| 54047.30238869876|              266|             55.93287312896705|((furniture, home...|           NSW|           Male|    78.02368530474212|    48572.882608193504| 0.11270526858914078|\n",
      "+------------+----------------+-------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating lag features to include previous month's revenue\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy('order_month_year')\n",
    "\n",
    "# Lagging features: Previous month's revenue\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'previous_month_revenue', F.lag('monthly_revenue', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate revenue growth (percentage change)\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'revenue_growth',\n",
    "    F.when(F.col('previous_month_revenue') > 0, \n",
    "           (F.col('monthly_revenue') - F.col('previous_month_revenue')) / F.col('previous_month_revenue'))\n",
    "    .otherwise(F.lit(0))  # Fill with 0 if there is no previous revenue\n",
    ")\n",
    "\n",
    "# Fill NA values for first month with 0 (no previous data available)\n",
    "monthly_revenue_df = monthly_revenue_df.fillna({'previous_month_revenue': 0, 'revenue_growth': 0})\n",
    "\n",
    "\n",
    "monthly_revenue_df = monthly_revenue_df.fillna(0)  # Filling NA values for first month\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/19 14:34:26 WARN DAGScheduler: Broadcasting large task binary with size 57.4 MiB\n",
      "24/09/19 14:34:32 WARN DAGScheduler: Broadcasting large task binary with size 57.4 MiB\n",
      "24/09/19 14:34:41 WARN DAGScheduler: Broadcasting large task binary with size 23.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+\n",
      "|merchant_abn|order_month_year|     scaled_features|\n",
      "+------------+----------------+--------------------+\n",
      "| 10023283211|          Apr-21|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Aug-21|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Dec-21|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Feb-22|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Jan-22|(439966,[0,1,2,3,...|\n",
      "+------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# StringIndexing categorical columns (merchant_tags, consumer_state, gender_consumer)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol='merchant_tags', outputCol='merchant_tags_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='state_consumer', outputCol='state_consumer_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='gender_consumer', outputCol='gender_consumer_indexed', handleInvalid='keep')\n",
    "]\n",
    "\n",
    "# OneHotEncoding indexed columns\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol='merchant_tags_indexed', outputCol='merchant_tags_encoded'),\n",
    "    OneHotEncoder(inputCol='state_consumer_indexed', outputCol='state_consumer_encoded'),\n",
    "    OneHotEncoder(inputCol='gender_consumer_indexed', outputCol='gender_consumer_encoded')\n",
    "]\n",
    "\n",
    "# VectorAssembler to combine numeric features into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'monthly_revenue', 'transaction_count', 'avg_fraud_probability_merchant', 'avg_unemployment_rate',\n",
    "        'merchant_tags_encoded', 'state_consumer_encoded', 'gender_consumer_encoded', 'revenue_growth'\n",
    "    ], \n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Standardizing the numeric features\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Fit the pipeline to the dataset\n",
    "model_pipeline = pipeline.fit(monthly_revenue_df)\n",
    "\n",
    "final_df = model_pipeline.transform(monthly_revenue_df)\n",
    "\n",
    "final_df.select('merchant_abn', 'order_month_year', 'scaled_features').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/19 14:34:56 WARN DAGScheduler: Broadcasting large task binary with size 80.1 MiB\n",
      "24/09/19 14:34:58 WARN DAGScheduler: Broadcasting large task binary with size 80.1 MiB\n",
      "24/09/19 14:35:03 WARN DAGScheduler: Broadcasting large task binary with size 84.5 MiB\n",
      "24/09/19 14:36:12 WARN DAGScheduler: Broadcasting large task binary with size 1728.3 KiB\n",
      "24/09/19 14:36:13 WARN DAGScheduler: Broadcasting large task binary with size 96.9 MiB\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_3 in memory! (computed 457.9 MiB so far)\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_5 in memory! (computed 457.9 MiB so far)\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_3 to disk instead.\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_5 to disk instead.\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_6 in memory! (computed 457.9 MiB so far)\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_6 to disk instead.\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_1 in memory! (computed 297.1 MiB so far)\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_1 to disk instead.\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_0 in memory! (computed 457.9 MiB so far)\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_0 to disk instead.\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_4 in memory! (computed 703.3 MiB so far)\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_4 to disk instead.\n",
      "24/09/19 14:36:18 WARN MemoryStore: Not enough space to cache rdd_443_7 in memory! (computed 457.9 MiB so far)\n",
      "24/09/19 14:36:18 WARN BlockManager: Persisting block rdd_443_7 to disk instead.\n",
      "24/09/19 14:36:30 WARN MemoryStore: Not enough space to cache rdd_443_2 in memory! (computed 3.6 GiB so far)\n",
      "24/09/19 14:36:30 WARN BlockManager: Persisting block rdd_443_2 to disk instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158.816s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 207.0 (TID 589): Retried waiting for GCLocker too often allocating 219985 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_5 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 ERROR Executor: Exception in task 5.0 in stage 207.0 (TID 589)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "24/09/19 14:36:30 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#95,Executor task launch worker for task 5.0 in stage 207.0 (TID 589),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "24/09/19 14:36:30 ERROR TaskSetManager: Task 5 in stage 207.0 failed 1 times; aborting job\n",
      "24/09/19 14:36:30 WARN BlockManager: Putting block rdd_443_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_4 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 4.0 in stage 207.0 (TID 588) (10.13.87.16 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/19 14:36:30 WARN BlockManager: Putting block rdd_443_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_6 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 WARN BlockManager: Putting block rdd_443_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_7 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 WARN BlockManager: Putting block rdd_443_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_1 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 WARN BlockManager: Putting block rdd_443_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 6.0 in stage 207.0 (TID 590) (10.13.87.16 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_3 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 WARN BlockManager: Putting block rdd_443_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/09/19 14:36:30 WARN BlockManager: Block rdd_443_0 could not be removed as it was not found on disk or in memory\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 7.0 in stage 207.0 (TID 591) (10.13.87.16 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 1.0 in stage 207.0 (TID 585) (10.13.87.16 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 3.0 in stage 207.0 (TID 587) (10.13.87.16 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/19 14:36:30 WARN TaskSetManager: Lost task 0.0 in stage 207.0 (TID 584) (10.13.87.16 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 207.0 failed 1 times, most recent failure: Lost task 5.0 in stage 207.0 (TID 589) (10.13.87.16 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.labeledPointToTreePoint(TreePoint.scala:91)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$.$anonfun$convertToTreeRDD$4(TreePoint.scala:75)\n",
      "\tat org.apache.spark.ml.tree.impl.TreePoint$$$Lambda/0x000000d8023fa950.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8012e1750.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x000000d8018c78a8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000000d801911d60.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\n",
      "Driver stacktrace:)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/nb/ptvngvk147s5wfkvdykjdlvm0000gn/T/ipykernel_4036/2418656622.py\", line 2, in <module>\n",
      "    rf_model = rf.fit(train_data)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonthly_revenue\u001b[39m\u001b[38;5;124m'\u001b[39m, maxBins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, maxDepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2116\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:556\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    550\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    551\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    553\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    557\u001b[0m }\n\u001b[1;32m    559\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/melissaputri/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(featuresCol='scaled_features', labelCol='monthly_revenue', maxBins=50, maxDepth=15)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# R-squared\n",
    "r2_evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='r2')\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "predictions.select('merchant_abn', 'order_month_year', 'monthly_revenue', 'revenue_growth','prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Future Monthly Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Parse the order_month_year column to a proper date format\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'order_month_year_date', F.to_date(F.concat(F.lit('01-'), F.col('order_month_year')), 'dd-MMM-yy')\n",
    ")\n",
    "\n",
    "# Get the most recent month per merchant\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy(F.desc('order_month_year_date'))\n",
    "latest_merchant_data = monthly_revenue_df.withColumn('row_num', F.row_number().over(window_spec)) \\\n",
    "                                         .filter(F.col('row_num') == 1) \\\n",
    "                                         .drop('row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_month = 'Aug-24'\n",
    "future_month_df = spark.createDataFrame([(next_month,)], ['future_order_month_year'])\n",
    "future_data = latest_merchant_data.crossJoin(future_month_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = model_pipeline.transform(future_data)\n",
    "future_data = rf_model.transform(future_data)\n",
    "future_predictions = future_data.select('merchant_abn', 'future_order_month_year', 'prediction')\n",
    "future_predictions = future_predictions.withColumnRenamed('prediction', 'projected_revenue')\n",
    "future_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_predictions = future_predictions.orderBy(F.col('projected_revenue').desc())\n",
    "\n",
    "# Show the top 10 merchants by predicted revenue\n",
    "top_10_predictions.select('merchant_abn', 'merchant_name', 'projected_revenue').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
