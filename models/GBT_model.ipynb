{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 14:36:18 WARN Utils: Your hostname, Melissas-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "24/09/20 14:36:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 14:36:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialise a spark session\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"GBT Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Increase driver memory\n",
    "    .config(\"spark.executor.memory\", \"16g\")  # Increase executor memory\n",
    "    .config(\"spark.executor.instances\", \"4\")  # Increase the number of executor instances\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction file\n",
    "transactions = spark.read.parquet('../data/curated/flagged_fraud')\n",
    "transactions = transactions.filter(F.col(\"is_fraud\") != True) # Exclude transactions marked as fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating monthly revenue for each merchant\n",
    "monthly_revenue_df = transactions.groupBy('merchant_abn', 'order_month_year').agg(\n",
    "    F.sum('dollar_value').alias('monthly_revenue'),\n",
    "    F.count('order_id').alias('transaction_count'),\n",
    "    F.avg('fraud_probability_merchant').alias('avg_fraud_probability_merchant'),\n",
    "    F.first('tags').alias('merchant_tags')  # Assuming tags are constant per merchant\n",
    ")\n",
    "    \n",
    "# Aggregating consumer-level features (most common state and gender for each merchant)\n",
    "\n",
    "# Most common consumer state per merchant\n",
    "consumer_state_mode = transactions.groupBy('merchant_abn', 'state_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'state_consumer')\n",
    "\n",
    "# Most common consumer gender per merchant\n",
    "consumer_gender_mode = transactions.groupBy('merchant_abn', 'gender_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'gender_consumer')\n",
    "\n",
    "# Average Unemployment Rate per Merchant Month-Year\n",
    "transactions = transactions.withColumn(\"unemployment_rate_numeric\", F.col(\"unemployment_rate\").cast(\"float\"))\n",
    "\n",
    "unemployment_agg = transactions.groupBy('merchant_abn', 'order_month_year').agg(\n",
    "    F.avg('unemployment_rate_numeric').alias('avg_unemployment_rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "|merchant_abn|order_month_year|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "| 10023283211|          Mar-21| 9076.307821688919|               40|             56.40749878739966|((furniture, home...|           NSW|           Male|     78.1724992275238|\n",
      "| 10142254217|          Nov-21| 13097.45235307313|              315|             55.47863229844303|([cable, satellit...|           NSW|           Male|    78.20253993745834|\n",
      "| 10187291046|          Jul-21| 906.4298127305271|                8|            56.333947346189554|([wAtch, clock, a...|           NSW|           Male|   60.799999713897705|\n",
      "| 10187291046|          May-21| 499.4454771066263|                8|             54.65314032706471|([wAtch, clock, a...|           NSW|           Male|    59.68750190734863|\n",
      "| 10187291046|          Nov-21|12097.857952854953|              105|            55.980061775518244|([wAtch, clock, a...|           NSW|           Male|    75.33428635370164|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining Datasets\n",
    "monthly_revenue_df = monthly_revenue_df.join(consumer_state_mode, on='merchant_abn', how='left') \\\n",
    "                                      .join(consumer_gender_mode, on='merchant_abn', how='left')\n",
    "\n",
    "# Join with unemployment data on both 'merchant_abn' and 'order_month_year'\n",
    "monthly_revenue_df = monthly_revenue_df.join(unemployment_agg, on=['merchant_abn', 'order_month_year'], how='left')\n",
    "\n",
    "# Show the final dataframe\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 14:36:31 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "|merchant_abn|order_month_year|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|previous_month_revenue|      revenue_growth|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "| 10023283211|          Apr-21|   9221.4058068711|               47|             56.03849374950703|((furniture, home...|           NSW|           Male|    74.54042625427246|                   0.0|                 0.0|\n",
      "| 10023283211|          Aug-21|15807.479921460477|               86|              56.2429773174978|((furniture, home...|           NSW|           Male|    80.79883797224178|       9221.4058068711|  0.7142158421964174|\n",
      "| 10023283211|          Dec-21| 66067.74432316715|              316|             55.74664488810393|((furniture, home...|           NSW|           Male|    76.78892362570461|    15807.479921460477|   3.179524165232218|\n",
      "| 10023283211|          Feb-22|48572.882608193504|              215|            56.069422789172336|((furniture, home...|           NSW|           Male|    71.28418544059576|     66067.74432316715|-0.26480186200089384|\n",
      "| 10023283211|          Jan-22| 54047.30238869876|              266|             55.93287312896705|((furniture, home...|           NSW|           Male|    78.02368530474212|    48572.882608193504| 0.11270526858914078|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating lag features to include previous month's revenue\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy('order_month_year')\n",
    "\n",
    "# Lagging features: Previous month's revenue\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'previous_month_revenue', F.lag('monthly_revenue', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate revenue growth (percentage change)\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'revenue_growth',\n",
    "    F.when(F.col('previous_month_revenue') > 0, \n",
    "           (F.col('monthly_revenue') - F.col('previous_month_revenue')) / F.col('previous_month_revenue'))\n",
    "    .otherwise(F.lit(0))  # Fill with 0 if there is no previous revenue\n",
    ")\n",
    "\n",
    "# Fill NA values for first month with 0 (no previous data available)\n",
    "monthly_revenue_df = monthly_revenue_df.fillna({'previous_month_revenue': 0, 'revenue_growth': 0})\n",
    "\n",
    "\n",
    "monthly_revenue_df = monthly_revenue_df.fillna(0)  # Filling NA values for first month\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 14:36:44 WARN DAGScheduler: Broadcasting large task binary with size 1443.5 KiB\n",
      "24/09/20 14:36:44 WARN DAGScheduler: Broadcasting large task binary with size 1433.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+\n",
      "|merchant_abn|order_month_year|     scaled_features|\n",
      "+------------+----------------+--------------------+\n",
      "| 10023283211|          Apr-21|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Aug-21|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Dec-21|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Feb-22|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Jan-22|(6698,[0,1,2,3,13...|\n",
      "+------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StringIndexing categorical columns (merchant_tags, consumer_state, gender_consumer)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol='merchant_tags', outputCol='merchant_tags_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='state_consumer', outputCol='state_consumer_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='gender_consumer', outputCol='gender_consumer_indexed', handleInvalid='keep')\n",
    "]\n",
    "\n",
    "# OneHotEncoding indexed columns\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol='merchant_tags_indexed', outputCol='merchant_tags_encoded'),\n",
    "    OneHotEncoder(inputCol='state_consumer_indexed', outputCol='state_consumer_encoded'),\n",
    "    OneHotEncoder(inputCol='gender_consumer_indexed', outputCol='gender_consumer_encoded')\n",
    "]\n",
    "\n",
    "# VectorAssembler to combine numeric features into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'monthly_revenue', 'transaction_count', 'avg_fraud_probability_merchant', 'avg_unemployment_rate',\n",
    "        'merchant_tags_encoded', 'state_consumer_encoded', 'gender_consumer_encoded', 'revenue_growth'\n",
    "    ], \n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Standardizing the numeric features\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Fit the pipeline to the dataset\n",
    "model_pipeline = pipeline.fit(monthly_revenue_df)\n",
    "\n",
    "final_df = model_pipeline.transform(monthly_revenue_df)\n",
    "\n",
    "final_df.select('merchant_abn', 'order_month_year', 'scaled_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 14:36:54 WARN DAGScheduler: Broadcasting large task binary with size 1871.9 KiB\n",
      "24/09/20 14:36:54 WARN DAGScheduler: Broadcasting large task binary with size 1872.0 KiB\n",
      "24/09/20 14:36:54 WARN DAGScheduler: Broadcasting large task binary with size 1942.9 KiB\n",
      "24/09/20 14:36:57 WARN DAGScheduler: Broadcasting large task binary with size 2014.7 KiB\n",
      "24/09/20 14:36:59 WARN DAGScheduler: Broadcasting large task binary with size 2015.4 KiB\n",
      "24/09/20 14:36:59 WARN DAGScheduler: Broadcasting large task binary with size 2016.2 KiB\n",
      "24/09/20 14:36:59 WARN DAGScheduler: Broadcasting large task binary with size 2017.1 KiB\n",
      "24/09/20 14:37:00 WARN DAGScheduler: Broadcasting large task binary with size 2019.4 KiB\n",
      "24/09/20 14:37:00 WARN DAGScheduler: Broadcasting large task binary with size 2026.3 KiB\n",
      "24/09/20 14:37:01 WARN DAGScheduler: Broadcasting large task binary with size 2026.8 KiB\n",
      "24/09/20 14:37:01 WARN DAGScheduler: Broadcasting large task binary with size 2027.5 KiB\n",
      "24/09/20 14:37:01 WARN DAGScheduler: Broadcasting large task binary with size 2028.5 KiB\n",
      "24/09/20 14:37:02 WARN DAGScheduler: Broadcasting large task binary with size 2030.7 KiB\n",
      "24/09/20 14:37:02 WARN DAGScheduler: Broadcasting large task binary with size 2032.7 KiB\n",
      "24/09/20 14:37:02 WARN DAGScheduler: Broadcasting large task binary with size 2033.2 KiB\n",
      "24/09/20 14:37:04 WARN DAGScheduler: Broadcasting large task binary with size 2033.9 KiB\n",
      "24/09/20 14:37:04 WARN DAGScheduler: Broadcasting large task binary with size 2034.9 KiB\n",
      "24/09/20 14:37:04 WARN DAGScheduler: Broadcasting large task binary with size 2037.0 KiB\n",
      "24/09/20 14:37:05 WARN DAGScheduler: Broadcasting large task binary with size 2038.5 KiB\n",
      "24/09/20 14:37:05 WARN DAGScheduler: Broadcasting large task binary with size 2039.0 KiB\n",
      "24/09/20 14:37:05 WARN DAGScheduler: Broadcasting large task binary with size 2039.7 KiB\n",
      "24/09/20 14:37:06 WARN DAGScheduler: Broadcasting large task binary with size 2040.7 KiB\n",
      "24/09/20 14:37:06 WARN DAGScheduler: Broadcasting large task binary with size 2042.3 KiB\n",
      "24/09/20 14:37:06 WARN DAGScheduler: Broadcasting large task binary with size 2043.9 KiB\n",
      "24/09/20 14:37:07 WARN DAGScheduler: Broadcasting large task binary with size 2044.3 KiB\n",
      "24/09/20 14:37:07 WARN DAGScheduler: Broadcasting large task binary with size 2045.1 KiB\n",
      "24/09/20 14:37:08 WARN DAGScheduler: Broadcasting large task binary with size 2046.0 KiB\n",
      "24/09/20 14:37:08 WARN DAGScheduler: Broadcasting large task binary with size 2047.9 KiB\n",
      "24/09/20 14:37:09 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:09 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:09 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:10 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:10 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:10 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:11 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:11 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:11 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:12 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:12 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:12 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:13 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:13 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:13 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:14 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:14 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:14 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:15 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:15 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:16 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:16 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:16 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:17 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:17 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:17 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:18 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:18 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:18 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:19 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:19 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:19 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:20 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:20 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:20 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 14:37:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:28 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:28 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:28 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 14:37:34 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    }
   ],
   "source": [
    "# Define the GBT Regressor\n",
    "gbt = GBTRegressor(featuresCol='scaled_features', labelCol='monthly_revenue')\n",
    "\n",
    "# Fit the model on the training data\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (GBT): 40026.50757559944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 14:37:37 WARN DAGScheduler: Broadcasting large task binary with size 1855.4 KiB\n",
      "24/09/20 14:37:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/09/20 14:37:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"RMSE (GBT): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.8374976778962497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 14:38:08 WARN DAGScheduler: Broadcasting large task binary with size 1855.4 KiB\n"
     ]
    }
   ],
   "source": [
    "r2_evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='r2')\n",
    "r2 = r2_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Parse the order_month_year column to a proper date format\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'order_month_year_date', F.to_date(F.concat(F.lit('01-'), F.col('order_month_year')), 'dd-MMM-yy')\n",
    ")\n",
    "\n",
    "# Get the most recent month per merchant\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy(F.desc('order_month_year_date'))\n",
    "latest_merchant_data = monthly_revenue_df.withColumn('row_num', F.row_number().over(window_spec)) \\\n",
    "                                         .filter(F.col('row_num') == 1) \\\n",
    "                                         .drop('row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_month = 'Aug-24'\n",
    "future_month_df = spark.createDataFrame([(next_month,)], ['future_order_month_year'])\n",
    "future_data = latest_merchant_data.crossJoin(future_month_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+---------------------+-----------------------+\n",
      "|merchant_abn|order_month_year|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|previous_month_revenue|      revenue_growth|order_month_year_date|future_order_month_year|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+---------------------+-----------------------+\n",
      "| 10023283211|          Feb-22|48572.882608193504|              215|            56.069422789172336|((furniture, home...|           NSW|           Male|    71.28418544059576|     66067.74432316715|-0.26480186200089384|           2022-02-01|                 Aug-24|\n",
      "| 10142254217|          Feb-22|  8761.41669580623|              218|            56.294407487052425|([cable, satellit...|           NSW|           Male|    76.60733880471746|     82.56213974011379|  105.11906042388328|           2022-02-01|                 Aug-24|\n",
      "| 10187291046|          Feb-22|  3165.48992443172|               26|             56.80884350094778|([wAtch, clock, a...|           NSW|           Male|    88.99230626913217|     4296.839080341215| -0.2632980045926351|           2022-02-01|                 Aug-24|\n",
      "| 10192359162|          Feb-22| 10514.66342879596|               23|            55.345751554313935|([music shops - m...|           NSW|           Male|    84.08260801564093|    21759.649496232232| -0.5167815809433596|           2022-02-01|                 Aug-24|\n",
      "| 10206519221|          Feb-22| 5828.400459934992|              145|             54.13255399137229|[(gift, card, nov...|           NSW|           Male|    71.57448196411133|    309.66984158167594|   17.82133704130094|           2022-02-01|                 Aug-24|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------+------------------+\n",
      "|merchant_abn|future_order_month_year| projected_revenue|\n",
      "+------------+-----------------------+------------------+\n",
      "| 10023283211|                 Aug-24| 56777.08567987866|\n",
      "| 10142254217|                 Aug-24| 7437.755257678557|\n",
      "| 10187291046|                 Aug-24|3563.1115237939453|\n",
      "| 10192359162|                 Aug-24|11864.291463825513|\n",
      "| 10206519221|                 Aug-24| 7437.755257678557|\n",
      "+------------+-----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future_data = model_pipeline.transform(future_data)\n",
    "future_data = gbt_model.transform(future_data)\n",
    "future_predictions = future_data.select('merchant_abn', 'future_order_month_year', 'prediction')\n",
    "future_predictions = future_predictions.withColumnRenamed('prediction', 'projected_revenue')\n",
    "future_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|merchant_abn|projected_revenue|\n",
      "+------------+-----------------+\n",
      "| 76626119831|995457.1811756655|\n",
      "| 77590625261|797461.5549158412|\n",
      "| 80518954462|797461.5549158412|\n",
      "| 79417999332|721028.5279519026|\n",
      "| 21439773999|718107.9921709804|\n",
      "| 86578477987|657879.2032864047|\n",
      "| 64403598239|596049.7554696605|\n",
      "| 38090089066|574001.2293418067|\n",
      "| 32361057556|555841.3133969607|\n",
      "| 43186523025|541156.3559394979|\n",
      "+------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_10_predictions = future_predictions.orderBy(F.col('projected_revenue').desc())\n",
    "\n",
    "# Show the top 10 merchants by predicted revenue\n",
    "top_10_predictions.select('merchant_abn', 'projected_revenue').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
