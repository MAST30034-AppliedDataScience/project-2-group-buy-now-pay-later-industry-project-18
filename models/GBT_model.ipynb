{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/20 15:14:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialise a spark session\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"GBT Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Increase driver memory\n",
    "    .config(\"spark.executor.memory\", \"16g\")  # Increase executor memory\n",
    "    .config(\"spark.executor.instances\", \"4\")  # Increase the number of executor instances\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction file\n",
    "transactions = spark.read.parquet('../data/curated/flagged_fraud')\n",
    "transactions = transactions.filter(F.col(\"is_fraud\") != True) # Exclude transactions marked as fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating monthly revenue for each merchant\n",
    "monthly_revenue_df = transactions.groupBy('merchant_abn', 'order_month_year').agg(\n",
    "    F.sum('dollar_value').alias('monthly_revenue'),\n",
    "    F.count('order_id').alias('transaction_count'),\n",
    "    F.avg('fraud_probability_merchant').alias('avg_fraud_probability_merchant'),\n",
    "    F.first('tags').alias('merchant_tags'),  # Assuming tags are constant per merchant\n",
    "    F.first('name_merchant').alias('merchant_name')\n",
    ")\n",
    "    \n",
    "# Aggregating consumer-level features (most common state and gender for each merchant)\n",
    "\n",
    "# Most common consumer state per merchant\n",
    "consumer_state_mode = transactions.groupBy('merchant_abn', 'state_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'state_consumer')\n",
    "\n",
    "# Most common consumer gender per merchant\n",
    "consumer_gender_mode = transactions.groupBy('merchant_abn', 'gender_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'gender_consumer')\n",
    "\n",
    "# Average Unemployment Rate per Merchant Month-Year\n",
    "transactions = transactions.withColumn(\"unemployment_rate_numeric\", F.col(\"unemployment_rate\").cast(\"float\"))\n",
    "\n",
    "unemployment_agg = transactions.groupBy('merchant_abn', 'order_month_year').agg(\n",
    "    F.avg('unemployment_rate_numeric').alias('avg_unemployment_rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------------+--------------+---------------+---------------------+\n",
      "|merchant_abn|order_month_year|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|       merchant_name|state_consumer|gender_consumer|avg_unemployment_rate|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------------+--------------+---------------+---------------------+\n",
      "| 10023283211|          Mar-21| 9076.307821688919|               40|             56.40749878739966|((furniture, home...|       Felis Limited|           NSW|           Male|     78.1724992275238|\n",
      "| 10142254217|          Nov-21| 13097.45235307313|              315|             55.47863229844303|([cable, satellit...|Arcu Ac Orci Corp...|           NSW|           Male|    78.20253993745834|\n",
      "| 10187291046|          Jul-21| 906.4298127305271|                8|            56.333947346189554|([wAtch, clock, a...|Ultricies Digniss...|           NSW|           Male|   60.799999713897705|\n",
      "| 10187291046|          May-21| 499.4454771066263|                8|             54.65314032706471|([wAtch, clock, a...|Ultricies Digniss...|           NSW|           Male|    59.68750190734863|\n",
      "| 10187291046|          Nov-21|12097.857952854953|              105|            55.980061775518244|([wAtch, clock, a...|Ultricies Digniss...|           NSW|           Male|    75.33428635370164|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------------+--------------+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining Datasets\n",
    "monthly_revenue_df = monthly_revenue_df.join(consumer_state_mode, on='merchant_abn', how='left') \\\n",
    "                                      .join(consumer_gender_mode, on='merchant_abn', how='left')\n",
    "\n",
    "# Join with unemployment data on both 'merchant_abn' and 'order_month_year'\n",
    "monthly_revenue_df = monthly_revenue_df.join(unemployment_agg, on=['merchant_abn', 'order_month_year'], how='left')\n",
    "\n",
    "# Show the final dataframe\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:14:33 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+-------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "|merchant_abn|order_month_year|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|merchant_name|state_consumer|gender_consumer|avg_unemployment_rate|previous_month_revenue|      revenue_growth|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+-------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "| 10023283211|          Apr-21|   9221.4058068711|               47|             56.03849374950703|((furniture, home...|Felis Limited|           NSW|           Male|    74.54042625427246|                   0.0|                 0.0|\n",
      "| 10023283211|          Aug-21|15807.479921460477|               86|              56.2429773174978|((furniture, home...|Felis Limited|           NSW|           Male|    80.79883797224178|       9221.4058068711|  0.7142158421964174|\n",
      "| 10023283211|          Dec-21| 66067.74432316715|              316|             55.74664488810393|((furniture, home...|Felis Limited|           NSW|           Male|    76.78892362570461|    15807.479921460477|   3.179524165232218|\n",
      "| 10023283211|          Feb-22|48572.882608193504|              215|            56.069422789172336|((furniture, home...|Felis Limited|           NSW|           Male|    71.28418544059576|     66067.74432316715|-0.26480186200089384|\n",
      "| 10023283211|          Jan-22| 54047.30238869876|              266|             55.93287312896705|((furniture, home...|Felis Limited|           NSW|           Male|    78.02368530474212|    48572.882608193504| 0.11270526858914078|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+-------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating lag features to include previous month's revenue\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy('order_month_year')\n",
    "\n",
    "# Lagging features: Previous month's revenue\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'previous_month_revenue', F.lag('monthly_revenue', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate revenue growth (percentage change)\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'revenue_growth',\n",
    "    F.when(F.col('previous_month_revenue') > 0, \n",
    "           (F.col('monthly_revenue') - F.col('previous_month_revenue')) / F.col('previous_month_revenue'))\n",
    "    .otherwise(F.lit(0))  # Fill with 0 if there is no previous revenue\n",
    ")\n",
    "\n",
    "# Fill NA values for first month with 0 (no previous data available)\n",
    "monthly_revenue_df = monthly_revenue_df.fillna({'previous_month_revenue': 0, 'revenue_growth': 0})\n",
    "\n",
    "\n",
    "monthly_revenue_df = monthly_revenue_df.fillna(0)  # Filling NA values for first month\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:14:49 WARN DAGScheduler: Broadcasting large task binary with size 1443.5 KiB\n",
      "24/09/20 15:14:50 WARN DAGScheduler: Broadcasting large task binary with size 1433.0 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+\n",
      "|merchant_abn|order_month_year|     scaled_features|\n",
      "+------------+----------------+--------------------+\n",
      "| 10023283211|          Apr-21|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Aug-21|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Dec-21|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Feb-22|(6698,[0,1,2,3,13...|\n",
      "| 10023283211|          Jan-22|(6698,[0,1,2,3,13...|\n",
      "+------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StringIndexing categorical columns (merchant_tags, consumer_state, gender_consumer)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol='merchant_tags', outputCol='merchant_tags_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='state_consumer', outputCol='state_consumer_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='gender_consumer', outputCol='gender_consumer_indexed', handleInvalid='keep')\n",
    "]\n",
    "\n",
    "# OneHotEncoding indexed columns\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol='merchant_tags_indexed', outputCol='merchant_tags_encoded'),\n",
    "    OneHotEncoder(inputCol='state_consumer_indexed', outputCol='state_consumer_encoded'),\n",
    "    OneHotEncoder(inputCol='gender_consumer_indexed', outputCol='gender_consumer_encoded')\n",
    "]\n",
    "\n",
    "# VectorAssembler to combine numeric features into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'monthly_revenue', 'transaction_count', 'avg_fraud_probability_merchant', 'avg_unemployment_rate',\n",
    "        'merchant_tags_encoded', 'state_consumer_encoded', 'gender_consumer_encoded', 'revenue_growth'\n",
    "    ], \n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Standardizing the numeric features\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Fit the pipeline to the dataset\n",
    "model_pipeline = pipeline.fit(monthly_revenue_df)\n",
    "\n",
    "final_df = model_pipeline.transform(monthly_revenue_df)\n",
    "\n",
    "final_df.select('merchant_abn', 'order_month_year', 'scaled_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:15:11 WARN DAGScheduler: Broadcasting large task binary with size 1869.1 KiB\n",
      "24/09/20 15:15:11 WARN DAGScheduler: Broadcasting large task binary with size 1869.2 KiB\n",
      "24/09/20 15:15:11 WARN DAGScheduler: Broadcasting large task binary with size 1940.0 KiB\n",
      "24/09/20 15:15:14 WARN DAGScheduler: Broadcasting large task binary with size 2011.1 KiB\n",
      "24/09/20 15:15:15 WARN DAGScheduler: Broadcasting large task binary with size 2011.9 KiB\n",
      "24/09/20 15:15:15 WARN DAGScheduler: Broadcasting large task binary with size 2012.6 KiB\n",
      "24/09/20 15:15:15 WARN DAGScheduler: Broadcasting large task binary with size 2013.6 KiB\n",
      "24/09/20 15:15:16 WARN DAGScheduler: Broadcasting large task binary with size 2015.9 KiB\n",
      "24/09/20 15:15:16 WARN DAGScheduler: Broadcasting large task binary with size 2022.6 KiB\n",
      "24/09/20 15:15:16 WARN DAGScheduler: Broadcasting large task binary with size 2023.1 KiB\n",
      "24/09/20 15:15:16 WARN DAGScheduler: Broadcasting large task binary with size 2023.8 KiB\n",
      "24/09/20 15:15:16 WARN DAGScheduler: Broadcasting large task binary with size 2024.8 KiB\n",
      "24/09/20 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 2026.5 KiB\n",
      "24/09/20 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 2028.2 KiB\n",
      "24/09/20 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 2028.7 KiB\n",
      "24/09/20 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 2029.4 KiB\n",
      "24/09/20 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 2030.3 KiB\n",
      "24/09/20 15:15:17 WARN DAGScheduler: Broadcasting large task binary with size 2032.0 KiB\n",
      "24/09/20 15:15:18 WARN DAGScheduler: Broadcasting large task binary with size 2033.5 KiB\n",
      "24/09/20 15:15:18 WARN DAGScheduler: Broadcasting large task binary with size 2034.0 KiB\n",
      "24/09/20 15:15:18 WARN DAGScheduler: Broadcasting large task binary with size 2034.7 KiB\n",
      "24/09/20 15:15:18 WARN DAGScheduler: Broadcasting large task binary with size 2035.7 KiB\n",
      "24/09/20 15:15:18 WARN DAGScheduler: Broadcasting large task binary with size 2037.6 KiB\n",
      "24/09/20 15:15:19 WARN DAGScheduler: Broadcasting large task binary with size 2039.0 KiB\n",
      "24/09/20 15:15:19 WARN DAGScheduler: Broadcasting large task binary with size 2039.5 KiB\n",
      "24/09/20 15:15:19 WARN DAGScheduler: Broadcasting large task binary with size 2040.2 KiB\n",
      "24/09/20 15:15:19 WARN DAGScheduler: Broadcasting large task binary with size 2041.2 KiB\n",
      "24/09/20 15:15:19 WARN DAGScheduler: Broadcasting large task binary with size 2043.1 KiB\n",
      "24/09/20 15:15:20 WARN DAGScheduler: Broadcasting large task binary with size 2044.7 KiB\n",
      "24/09/20 15:15:20 WARN DAGScheduler: Broadcasting large task binary with size 2045.2 KiB\n",
      "24/09/20 15:15:20 WARN DAGScheduler: Broadcasting large task binary with size 2045.9 KiB\n",
      "24/09/20 15:15:20 WARN DAGScheduler: Broadcasting large task binary with size 2046.8 KiB\n",
      "24/09/20 15:15:20 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:23 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:24 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:25 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:27 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:28 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:29 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:29 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:29 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:29 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:29 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/09/20 15:15:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:31 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/09/20 15:15:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    }
   ],
   "source": [
    "# Define the GBT Regressor\n",
    "gbt = GBTRegressor(featuresCol='scaled_features', labelCol='monthly_revenue')\n",
    "\n",
    "# Fit the model on the training data\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (GBT): 36676.25817780103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:15:36 WARN DAGScheduler: Broadcasting large task binary with size 1857.9 KiB\n",
      "24/09/20 15:15:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/09/20 15:15:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"RMSE (GBT): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.8230838912823306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:15:39 WARN DAGScheduler: Broadcasting large task binary with size 1857.9 KiB\n"
     ]
    }
   ],
   "source": [
    "r2_evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='r2')\n",
    "r2 = r2_evaluator.evaluate(gbt_predictions)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Parse the order_month_year column to a proper date format\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'order_month_year_date', F.to_date(F.concat(F.lit('01-'), F.col('order_month_year')), 'dd-MMM-yy')\n",
    ")\n",
    "\n",
    "# Get the most recent month per merchant\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy(F.desc('order_month_year_date'))\n",
    "latest_merchant_data = monthly_revenue_df.withColumn('row_num', F.row_number().over(window_spec)) \\\n",
    "                                         .filter(F.col('row_num') == 1) \\\n",
    "                                         .drop('row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_month = 'Aug-24'\n",
    "future_month_df = spark.createDataFrame([(next_month,)], ['future_order_month_year'])\n",
    "future_data = latest_merchant_data.crossJoin(future_month_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+---------------------+-----------------------+\n",
      "|merchant_abn|order_month_year|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|       merchant_name|state_consumer|gender_consumer|avg_unemployment_rate|previous_month_revenue|      revenue_growth|order_month_year_date|future_order_month_year|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+---------------------+-----------------------+\n",
      "| 10023283211|          Feb-22|48572.882608193504|              215|            56.069422789172336|((furniture, home...|       Felis Limited|           NSW|           Male|    71.28418544059576|     66067.74432316715|-0.26480186200089384|           2022-02-01|                 Aug-24|\n",
      "| 10142254217|          Feb-22|  8761.41669580623|              218|            56.294407487052425|([cable, satellit...|Arcu Ac Orci Corp...|           NSW|           Male|    76.60733880471746|     82.56213974011379|  105.11906042388328|           2022-02-01|                 Aug-24|\n",
      "| 10187291046|          Feb-22|  3165.48992443172|               26|             56.80884350094778|([wAtch, clock, a...|Ultricies Digniss...|           NSW|           Male|    88.99230626913217|     4296.839080341215| -0.2632980045926351|           2022-02-01|                 Aug-24|\n",
      "| 10192359162|          Feb-22| 10514.66342879596|               23|            55.345751554313935|([music shops - m...| Enim Condimentum PC|           NSW|           Male|    84.08260801564093|    21759.649496232232| -0.5167815809433596|           2022-02-01|                 Aug-24|\n",
      "| 10206519221|          Feb-22| 5828.400459934992|              145|             54.13255399137229|[(gift, card, nov...|       Fusce Company|           NSW|           Male|    71.57448196411133|    309.66984158167594|   17.82133704130094|           2022-02-01|                 Aug-24|\n",
      "+------------+----------------+------------------+-----------------+------------------------------+--------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+-----------------------+-----------------+\n",
      "|merchant_abn|       merchant_name|       merchant_tags|future_order_month_year|projected_revenue|\n",
      "+------------+--------------------+--------------------+-----------------------+-----------------+\n",
      "| 10023283211|       Felis Limited|((furniture, home...|                 Aug-24|37185.27616752366|\n",
      "| 10142254217|Arcu Ac Orci Corp...|([cable, satellit...|                 Aug-24|9640.759904114244|\n",
      "| 10187291046|Ultricies Digniss...|([wAtch, clock, a...|                 Aug-24|2084.518863219004|\n",
      "| 10192359162| Enim Condimentum PC|([music shops - m...|                 Aug-24|9640.759904114244|\n",
      "| 10206519221|       Fusce Company|[(gift, card, nov...|                 Aug-24|2084.518863219004|\n",
      "+------------+--------------------+--------------------+-----------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future_data = model_pipeline.transform(future_data)\n",
    "future_data = gbt_model.transform(future_data)\n",
    "future_predictions = future_data.select('merchant_abn', 'merchant_name', 'merchant_tags','future_order_month_year', 'prediction')\n",
    "future_predictions = future_predictions.withColumnRenamed('prediction', 'projected_revenue')\n",
    "future_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+-----------------------+------------------+\n",
      "|merchant_abn|       merchant_name|       merchant_tags|future_order_month_year| projected_revenue|\n",
      "+------------+--------------------+--------------------+-----------------------+------------------+\n",
      "| 35909341340|Arcu Sed Eu Incor...|[(computer progra...|                 Aug-24|  853384.885861524|\n",
      "| 21439773999|Mauris Non Institute|([cable, satellit...|                 Aug-24| 524889.8413835187|\n",
      "| 90543168331|Phasellus Dapibus...|([furniture, home...|                 Aug-24| 506924.3873193684|\n",
      "| 32361057556|Orci In Consequat...|([gift, card, nov...|                 Aug-24|488698.90051381686|\n",
      "| 52959528548|   Libero Et Limited|[[furniture, home...|                 Aug-24|475910.65461107524|\n",
      "| 68559320474|Aliquam Auctor As...|([antique shops -...|                 Aug-24| 460857.8096890968|\n",
      "| 76767266140|Phasellus At Limited|((furniture, home...|                 Aug-24|460717.10870484094|\n",
      "| 79417999332|Phasellus At Company|([gift, card, nov...|                 Aug-24| 458992.0647102518|\n",
      "| 98973094975|   Ornare Fusce Inc.|[(hobby, toy and ...|                 Aug-24| 455924.3488945005|\n",
      "| 94493496784|Dictum Phasellus ...|[(gift, card, nov...|                 Aug-24| 431658.4146071128|\n",
      "+------------+--------------------+--------------------+-----------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GBT_predictions = future_predictions.orderBy(F.col('projected_revenue').desc())\n",
    "\n",
    "# Show the top 10 merchants by predicted revenue\n",
    "GBT_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 15:16:11 WARN DAGScheduler: Broadcasting large task binary with size 1703.3 KiB\n"
     ]
    }
   ],
   "source": [
    "GBT_predictions.write.parquet('GBT_ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
