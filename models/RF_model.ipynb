{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 14:32:58 WARN Utils: Your hostname, Melissas-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "24/09/25 14:32:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/25 14:32:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialise a spark session\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"RF Model\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Increase driver memory\n",
    "    .config(\"spark.executor.memory\", \"16g\")  # Increase executor memory\n",
    "    .config(\"spark.executor.instances\", \"4\")  # Increase the number of executor instances\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction file\n",
    "transactions = spark.read.parquet('../data/curated/flagged_fraud')\n",
    "transactions = transactions.filter(F.col(\"is_fraud\") != True) # Exclude transactions marked as fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 14:33:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Aggregating monthly revenue for each merchant\n",
    "monthly_revenue_df = transactions.groupBy('merchant_abn', 'order_month_year', 'name_merchant').agg(\n",
    "    F.sum('dollar_value').alias('monthly_revenue'),\n",
    "    F.count('order_id').alias('transaction_count'),\n",
    "    F.avg('fraud_probability_merchant').alias('avg_fraud_probability_merchant'),\n",
    "    F.first('tags').alias('merchant_tags')  # Assuming tags are constant per merchant\n",
    ")\n",
    "    \n",
    "# Aggregating consumer-level features (most common state and gender for each merchant)\n",
    "\n",
    "# Most common consumer state per merchant\n",
    "consumer_state_mode = transactions.groupBy('merchant_abn', 'state_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'state_consumer')\n",
    "\n",
    "# Most common consumer gender per merchant\n",
    "consumer_gender_mode = transactions.groupBy('merchant_abn', 'gender_consumer').count() \\\n",
    "    .withColumn('row_num', F.row_number().over(Window.partitionBy('merchant_abn').orderBy(F.desc('count')))) \\\n",
    "    .filter(F.col('row_num') == 1) \\\n",
    "    .select('merchant_abn', 'gender_consumer')\n",
    "\n",
    "# Average Unemployment Rate per Merchant Month-Year\n",
    "transactions = transactions.withColumn(\"unemployment_rate_numeric\", F.col(\"unemployment_rate\").cast(\"float\"))\n",
    "\n",
    "unemployment_agg = transactions.groupBy('merchant_abn', 'order_month_year').agg(\n",
    "    F.avg('unemployment_rate_numeric').alias('avg_unemployment_rate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "|merchant_abn|order_month_year|       name_merchant|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|\n",
      "+------------+----------------+--------------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "| 10023283211|          Aug-21|       Felis Limited|15807.479921460477|               86|              56.2429773174978|((furniture, home...|           NSW|           Male|    80.79883797224178|\n",
      "| 10023283211|          Mar-21|       Felis Limited| 9076.307821688919|               40|             56.40749878739966|((furniture, home...|           NSW|           Male|     78.1724992275238|\n",
      "| 10023283211|          May-21|       Felis Limited|11953.898144671877|               47|             55.55771129303114|((furniture, home...|           NSW|           Male|    77.15957534059565|\n",
      "| 10142254217|          Feb-21|Arcu Ac Orci Corp...| 82.56213974011379|                2|             57.58330762068199|([cable, satellit...|           NSW|           Male|    63.44999885559082|\n",
      "| 10142254217|          May-21|Arcu Ac Orci Corp...|2031.1632147275016|               68|             55.92988477860093|([cable, satellit...|           NSW|           Male|    74.11911787706264|\n",
      "+------------+----------------+--------------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining Datasets\n",
    "monthly_revenue_df = monthly_revenue_df.join(consumer_state_mode, on='merchant_abn', how='left') \\\n",
    "                                      .join(consumer_gender_mode, on='merchant_abn', how='left')\n",
    "\n",
    "# Join with unemployment data on both 'merchant_abn' and 'order_month_year'\n",
    "monthly_revenue_df = monthly_revenue_df.join(unemployment_agg, on=['merchant_abn', 'order_month_year'], how='left')\n",
    "\n",
    "# Show the final dataframe\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "|merchant_abn|order_month_year|name_merchant|   monthly_revenue|transaction_count|avg_fraud_probability_merchant|       merchant_tags|state_consumer|gender_consumer|avg_unemployment_rate|previous_month_revenue|      revenue_growth|\n",
      "+------------+----------------+-------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "| 10023283211|          Apr-21|Felis Limited|   9221.4058068711|               47|             56.03849374950703|((furniture, home...|           NSW|           Male|    74.54042625427246|                   0.0|                 0.0|\n",
      "| 10023283211|          Aug-21|Felis Limited|15807.479921460477|               86|              56.2429773174978|((furniture, home...|           NSW|           Male|    80.79883797224178|       9221.4058068711|  0.7142158421964174|\n",
      "| 10023283211|          Dec-21|Felis Limited| 66067.74432316715|              316|             55.74664488810393|((furniture, home...|           NSW|           Male|    76.78892362570461|    15807.479921460477|   3.179524165232218|\n",
      "| 10023283211|          Feb-22|Felis Limited|48572.882608193504|              215|            56.069422789172336|((furniture, home...|           NSW|           Male|    71.28418544059576|     66067.74432316715|-0.26480186200089384|\n",
      "| 10023283211|          Jan-22|Felis Limited| 54047.30238869876|              266|             55.93287312896705|((furniture, home...|           NSW|           Male|    78.02368530474212|    48572.882608193504| 0.11270526858914078|\n",
      "+------------+----------------+-------------+------------------+-----------------+------------------------------+--------------------+--------------+---------------+---------------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating lag features to include previous month's revenue\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy('order_month_year')\n",
    "\n",
    "# Lagging features: Previous month's revenue\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'previous_month_revenue', F.lag('monthly_revenue', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate revenue growth (percentage change)\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'revenue_growth',\n",
    "    F.when(F.col('previous_month_revenue') > 0, \n",
    "           (F.col('monthly_revenue') - F.col('previous_month_revenue')) / F.col('previous_month_revenue'))\n",
    "    .otherwise(F.lit(0))  # Fill with 0 if there is no previous revenue\n",
    ")\n",
    "\n",
    "# Fill NA values for first month with 0 (no previous data available)\n",
    "monthly_revenue_df = monthly_revenue_df.fillna({'previous_month_revenue': 0, 'revenue_growth': 0})\n",
    "\n",
    "\n",
    "monthly_revenue_df = monthly_revenue_df.fillna(0)  # Filling NA values for first month\n",
    "monthly_revenue_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 14:34:04 WARN DAGScheduler: Broadcasting large task binary with size 57.4 MiB\n",
      "24/09/25 14:34:12 WARN DAGScheduler: Broadcasting large task binary with size 57.4 MiB\n",
      "24/09/25 14:34:25 WARN DAGScheduler: Broadcasting large task binary with size 23.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+--------------------+\n",
      "|merchant_abn|order_month_year|     scaled_features|\n",
      "+------------+----------------+--------------------+\n",
      "| 10023283211|          Apr-21|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Aug-21|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Dec-21|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Feb-22|(439966,[0,1,2,3,...|\n",
      "| 10023283211|          Jan-22|(439966,[0,1,2,3,...|\n",
      "+------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# StringIndexing categorical columns (merchant_tags, consumer_state, gender_consumer)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol='merchant_tags', outputCol='merchant_tags_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='state_consumer', outputCol='state_consumer_indexed', handleInvalid='keep'),\n",
    "    StringIndexer(inputCol='gender_consumer', outputCol='gender_consumer_indexed', handleInvalid='keep')\n",
    "]\n",
    "\n",
    "# OneHotEncoding indexed columns\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol='merchant_tags_indexed', outputCol='merchant_tags_encoded'),\n",
    "    OneHotEncoder(inputCol='state_consumer_indexed', outputCol='state_consumer_encoded'),\n",
    "    OneHotEncoder(inputCol='gender_consumer_indexed', outputCol='gender_consumer_encoded')\n",
    "]\n",
    "\n",
    "# VectorAssembler to combine numeric features into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'monthly_revenue', 'transaction_count', 'avg_fraud_probability_merchant', 'avg_unemployment_rate',\n",
    "        'merchant_tags_encoded', 'state_consumer_encoded', 'gender_consumer_encoded', 'revenue_growth'\n",
    "    ], \n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Standardizing the numeric features\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "\n",
    "# Fit the pipeline to the dataset\n",
    "model_pipeline = pipeline.fit(monthly_revenue_df)\n",
    "\n",
    "final_df = model_pipeline.transform(monthly_revenue_df)\n",
    "\n",
    "final_df.select('merchant_abn', 'order_month_year', 'scaled_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RandomForestRegressor model\n",
    "rf = RandomForestRegressor(featuresCol='scaled_features', labelCol='monthly_revenue')\n",
    "\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [20, 50, 100])     # Tune number of trees\n",
    "             .addGrid(rf.maxDepth, [5, 10, 15])       # Tune max depth of the tree\n",
    "             .addGrid(rf.maxBins, [32, 50])           # Tune number of bins\n",
    "             .build())\n",
    "\n",
    "# Define an evaluator based on RMSE\n",
    "evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Set up cross-validation with 5 folds\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 14:35:05 WARN DAGScheduler: Broadcasting large task binary with size 80.2 MiB\n",
      "24/09/25 14:36:42 WARN DAGScheduler: Broadcasting large task binary with size 80.2 MiB\n",
      "24/09/25 14:36:44 WARN DAGScheduler: Broadcasting large task binary with size 80.2 MiB\n",
      "24/09/25 14:38:07 WARN DAGScheduler: Broadcasting large task binary with size 84.5 MiB\n",
      "24/09/25 14:40:53 WARN DAGScheduler: Broadcasting large task binary with size 1728.3 KiB\n",
      "24/09/25 14:40:56 WARN DAGScheduler: Broadcasting large task binary with size 97.0 MiB\n",
      "24/09/25 14:41:10 WARN MemoryStore: Not enough space to cache rdd_407_2 in memory! (computed 3.6 GiB so far)\n",
      "24/09/25 14:41:11 WARN MemoryStore: Not enough space to cache rdd_407_0 in memory! (computed 3.6 GiB so far)\n",
      "24/09/25 14:41:11 WARN MemoryStore: Not enough space to cache rdd_407_8 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:41:11 WARN BlockManager: Persisting block rdd_407_0 to disk instead.\n",
      "24/09/25 14:41:11 WARN BlockManager: Persisting block rdd_407_2 to disk instead.\n",
      "24/09/25 14:41:11 WARN BlockManager: Persisting block rdd_407_8 to disk instead.\n",
      "24/09/25 14:41:11 WARN MemoryStore: Not enough space to cache rdd_407_11 in memory! (computed 55.4 MiB so far)\n",
      "24/09/25 14:41:11 WARN BlockManager: Persisting block rdd_407_11 to disk instead.\n",
      "24/09/25 14:41:11 WARN MemoryStore: Not enough space to cache rdd_407_9 in memory! (computed 297.1 MiB so far)\n",
      "24/09/25 14:41:11 WARN BlockManager: Persisting block rdd_407_9 to disk instead.\n",
      "24/09/25 14:41:11 WARN MemoryStore: Not enough space to cache rdd_407_10 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:11 WARN BlockManager: Persisting block rdd_407_10 to disk instead.\n",
      "24/09/25 14:41:17 WARN MemoryStore: Not enough space to cache rdd_407_12 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:17 WARN BlockManager: Persisting block rdd_407_12 to disk instead.\n",
      "24/09/25 14:41:17 WARN MemoryStore: Not enough space to cache rdd_407_13 in memory! (computed 55.4 MiB so far)\n",
      "24/09/25 14:41:17 WARN BlockManager: Persisting block rdd_407_13 to disk instead.\n",
      "24/09/25 14:41:18 WARN MemoryStore: Not enough space to cache rdd_407_12 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:41:18 WARN MemoryStore: Not enough space to cache rdd_407_8 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:41:22 WARN MemoryStore: Not enough space to cache rdd_407_9 in memory! (computed 55.4 MiB so far)\n",
      "24/09/25 14:41:37 WARN MemoryStore: Not enough space to cache rdd_407_0 in memory! (computed 2.4 GiB so far)\n",
      "24/09/25 14:41:37 WARN MemoryStore: Not enough space to cache rdd_407_2 in memory! (computed 3.6 GiB so far)\n",
      "24/09/25 14:41:46 WARN MemoryStore: Not enough space to cache rdd_407_16 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:46 WARN BlockManager: Persisting block rdd_407_16 to disk instead.\n",
      "24/09/25 14:41:46 WARN MemoryStore: Not enough space to cache rdd_407_17 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:46 WARN BlockManager: Persisting block rdd_407_17 to disk instead.\n",
      "24/09/25 14:41:47 WARN MemoryStore: Not enough space to cache rdd_407_17 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:50 WARN MemoryStore: Not enough space to cache rdd_407_18 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:50 WARN BlockManager: Persisting block rdd_407_18 to disk instead.\n",
      "24/09/25 14:41:51 WARN MemoryStore: Not enough space to cache rdd_407_18 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:53 WARN MemoryStore: Not enough space to cache rdd_407_10 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:59 WARN MemoryStore: Not enough space to cache rdd_407_19 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:41:59 WARN BlockManager: Persisting block rdd_407_19 to disk instead.\n",
      "24/09/25 14:42:00 WARN MemoryStore: Not enough space to cache rdd_407_20 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:42:00 WARN BlockManager: Persisting block rdd_407_20 to disk instead.\n",
      "24/09/25 14:42:05 WARN MemoryStore: Not enough space to cache rdd_407_22 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:42:05 WARN BlockManager: Persisting block rdd_407_22 to disk instead.\n",
      "24/09/25 14:42:10 WARN MemoryStore: Not enough space to cache rdd_407_22 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:42:11 WARN MemoryStore: Not enough space to cache rdd_407_23 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:42:11 WARN BlockManager: Persisting block rdd_407_23 to disk instead.\n",
      "24/09/25 14:42:12 WARN MemoryStore: Not enough space to cache rdd_407_23 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:42:39 WARN MemoryStore: Not enough space to cache rdd_407_26 in memory! (computed 55.4 MiB so far)\n",
      "24/09/25 14:42:39 WARN BlockManager: Persisting block rdd_407_26 to disk instead.\n",
      "24/09/25 14:42:43 WARN MemoryStore: Not enough space to cache rdd_407_26 in memory! (computed 55.4 MiB so far)\n",
      "24/09/25 14:42:44 WARN MemoryStore: Not enough space to cache rdd_407_25 in memory! (computed 2.4 GiB so far)\n",
      "24/09/25 14:42:44 WARN BlockManager: Persisting block rdd_407_25 to disk instead.\n",
      "24/09/25 14:43:08 WARN MemoryStore: Not enough space to cache rdd_407_28 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:43:08 WARN BlockManager: Persisting block rdd_407_28 to disk instead.\n",
      "24/09/25 14:43:08 WARN MemoryStore: Not enough space to cache rdd_407_25 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:43:09 WARN MemoryStore: Not enough space to cache rdd_407_13 in memory! (computed 5.5 GiB so far)\n",
      "24/09/25 14:43:09 WARN MemoryStore: Not enough space to cache rdd_407_29 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:43:09 WARN BlockManager: Persisting block rdd_407_29 to disk instead.\n",
      "24/09/25 14:43:09 WARN MemoryStore: Not enough space to cache rdd_407_30 in memory! (computed 55.4 MiB so far)\n",
      "24/09/25 14:43:09 WARN BlockManager: Persisting block rdd_407_30 to disk instead.\n",
      "24/09/25 14:43:09 WARN MemoryStore: Not enough space to cache rdd_407_20 in memory! (computed 28.5 MiB so far)\n",
      "24/09/25 14:43:12 WARN MemoryStore: Not enough space to cache rdd_407_28 in memory! (computed 109.1 MiB so far)\n",
      "24/09/25 14:43:20 WARN MemoryStore: Not enough space to cache rdd_407_29 in memory! (computed 297.1 MiB so far)\n",
      "24/09/25 14:43:21 WARN MemoryStore: Not enough space to cache rdd_407_30 in memory! (computed 28.5 MiB so far)\n",
      "24/09/25 14:43:22 WARN MemoryStore: Not enough space to cache rdd_407_31 in memory! (computed 28.5 MiB so far)\n",
      "24/09/25 14:43:22 WARN BlockManager: Persisting block rdd_407_31 to disk instead.\n",
      "24/09/25 14:43:25 WARN MemoryStore: Not enough space to cache rdd_407_31 in memory! (computed 28.5 MiB so far)\n",
      "24/09/25 14:43:46 WARN MemoryStore: Not enough space to cache rdd_407_33 in memory! (computed 189.7 MiB so far)\n",
      "24/09/25 14:43:46 WARN BlockManager: Persisting block rdd_407_33 to disk instead.\n",
      "24/09/25 14:43:53 WARN MemoryStore: Not enough space to cache rdd_407_33 in memory! (computed 189.7 MiB so far)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the cross-validated model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m crossval\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the cross-validated model\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cvModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RMSE on the test data\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Evaluate R-squared on the test data\n",
    "r2_evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='r2')\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Show a few predictions\n",
    "predictions.select('merchant_abn', 'order_month_year', 'monthly_revenue', 'revenue_growth', 'prediction').show(5)\n",
    "\n",
    "# Best model's parameters\n",
    "best_model = cvModel.bestModel\n",
    "print(f\"Best numTrees: {best_model.getNumTrees}\")\n",
    "print(f\"Best maxDepth: {best_model.getMaxDepth}\")\n",
    "print(f\"Best maxBins: {best_model.getMaxBins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Parse the order_month_year column to a proper date format\n",
    "monthly_revenue_df = monthly_revenue_df.withColumn(\n",
    "    'order_month_year_date', F.to_date(F.concat(F.lit('01-'), F.col('order_month_year')), 'dd-MMM-yy')\n",
    ")\n",
    "\n",
    "# Get the most recent month per merchant\n",
    "window_spec = Window.partitionBy('merchant_abn').orderBy(F.desc('order_month_year_date'))\n",
    "latest_merchant_data = monthly_revenue_df.withColumn('row_num', F.row_number().over(window_spec)) \\\n",
    "                                         .filter(F.col('row_num') == 1) \\\n",
    "                                         .drop('row_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_month = 'Aug-24'\n",
    "future_month_df = spark.createDataFrame([(next_month,)], ['future_order_month_year'])\n",
    "future_data = latest_merchant_data.crossJoin(future_month_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = model_pipeline.transform(future_data)\n",
    "future_data = cvModel.transform(future_data)\n",
    "future_predictions = future_data.select('merchant_abn', 'merchant_name','merchant_tags','future_order_month_year', 'prediction')\n",
    "future_predictions = future_predictions.withColumnRenamed('prediction', 'projected_revenue')\n",
    "future_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_predictions = future_predictions.orderBy(F.col('projected_revenue').desc())\n",
    "\n",
    "# Show the top 10 merchants by predicted revenue\n",
    "top_10_predictions.select('merchant_abn', 'merchant_name', 'projected_revenue').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/25 14:44:01 ERROR Instrumentation: org.apache.spark.SparkException: Job 92 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "24/09/25 14:44:01 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:611)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:563)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:577)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:577)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:417)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:402)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
